{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947e40da",
   "metadata": {},
   "source": [
    "# Exercise: Model Learning\n",
    "\n",
    "Our exercise includes:\n",
    "- selecting and eliminating features\n",
    "- hyperparameters\n",
    "- cross-validation\n",
    "\n",
    "## Set Up\n",
    "\n",
    "### Extract\n",
    "\n",
    "1\\. Import `rpg-characters.csv` to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5671f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb6565",
   "metadata": {},
   "source": [
    "2\\. Focus on a dependent variable. It can be whatever you like. Then separate the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f581f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent/independent variable, target/features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc21e7",
   "metadata": {},
   "source": [
    "3\\. Use `train_test_split`. The testing set is 25% of our dataset. We can choose `random_state`, though it needs to be a constant value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b06de39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1d574",
   "metadata": {},
   "source": [
    "### Lasso (Least Absolute Shrinkage and Selection Operator)\n",
    "\n",
    "[Lasso Linear Model](https://scikit-learn.org/stable/modules/linear_model.html#lasso)\n",
    "\n",
    "[LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)\n",
    "\n",
    "1\\. Use the `LassoCV` model.\n",
    "\n",
    "`alphas` can go from general-purpose, heavy, light, or manual. Play around with it.\n",
    "\n",
    "```py\n",
    "# General-purpose regression: alphas=np.logspace(-4, 1, 50-100)\n",
    "# Heavy regularization (sparse model): alphas=np.logspace(0, 3, 50)\n",
    "# Light regularization: alphas=np.logspace(-6, 0, 50)\n",
    "# Manual tuning: alphas=[0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "lasso = LassoCV(\n",
    "    alphas=np.logspace(-4, 1, 100), # general-purpose\n",
    "    max_iter=100000,\n",
    ")\n",
    "```\n",
    "\n",
    "There are no multiple dependent variables with `LassoCV`. It has to be a single dependent variable.\n",
    "\n",
    "```\n",
    "Dependent variables: weight, height, score ❌\n",
    "Single dependent variable: score ✅\n",
    "Single dependent variable: strength ✅\n",
    "Single dependent variable: modifier1 ✅\n",
    "```\n",
    "\n",
    "Post `fit`, feature coefficients equal to `0` will be removed, feature coefficients not equal to `0` are selected, and pick the \"best\" alpha: `lasso.alpha_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b36bf3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "# removed features\n",
    "# print(f\"Removed Features: {(lasso.coef_ == 0).sum()}\") or print(f\"Removed Features: {np.sum(lasso.coef_ == 0)}\")\n",
    "\n",
    "# selected features\n",
    "# print(f\"Selected Features: {X_train.columns[lasso.coef_ != 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac88d9",
   "metadata": {},
   "source": [
    "2\\. Use the `predict` method on the test set. Print the R<sup>2</sup> [.score](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV.score) and the median absolute error [median_absolute_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ff17d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso.predict\n",
    "# lasso.score\n",
    "# mae = median_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209938bc",
   "metadata": {},
   "source": [
    "3\\. Build a scatter plot: predicted versus actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "485c91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted versus actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35101aec",
   "metadata": {},
   "source": [
    "4\\. Build a scatter plot: residuals versus predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3f37412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals versus predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03976a",
   "metadata": {},
   "source": [
    "### LinearRegression\n",
    "\n",
    "Use the same dataset on our `LinearRegression`.\n",
    "\n",
    "- `fit`\n",
    "- `predict`\n",
    "- Print the R^2 score.\n",
    "- Print the `median_absolute_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88373b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f4e07",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "\n",
    "Ridge is an algorithm that detects highly correlated independent variables. It's useful for multicollinearity.\n",
    "\n",
    "[Ridge Linear Model](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification)\n",
    "\n",
    "[RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)\n",
    "\n",
    "Use the `RidgeCV` model on the same dataset.\n",
    "\n",
    "`alphas` can go from general-purpose, heavy, light, or manual. Play around with it. With the `fit` method, we want to pick the \"best\" alpha: `ridge.alpha_`.\n",
    "\n",
    "```py\n",
    "# General-purpose regression: alphas=np.logspace(-4, 1, 50-100)\n",
    "# Heavy regularization (sparse model): alphas=np.logspace(0, 3, 50)\n",
    "# Light regularization: alphas=np.logspace(-6, 0, 50)\n",
    "# Manual tuning: alphas=[0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "ridge = RidgeCV(\n",
    "    alphas=np.logspace(-4, 1, 100), # general-purpose\n",
    ")\n",
    "```\n",
    "\n",
    "There _are_ multiple dependent variables with `RidgeCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46265723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2a524",
   "metadata": {},
   "source": [
    "## Reflect\n",
    "\n",
    "Pick from 3-5 dependent variables. Which models (`LassoCV`, `LinearRegression`, and `RidgeCV`) outperform the R<sup>2</sup> score? Are they the same?\n",
    "\n",
    "Focus on the dependent `score float64` variable. Those models are aggressively correlated to the independent variables. Which should you pick: `LassoCV`, `LinearRegression`, `RidgeCV`?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week-10 (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
